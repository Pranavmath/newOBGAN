{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TENkdZFjKhgb",
    "outputId": "83e567c5-f743-47d3-9539-c00c6beea199"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmdbfMPEMjHD",
    "outputId": "504327fb-b58a-4b93-a696-04b3ec69e054"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Pranavmath/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmgGdvjVCSfN",
    "outputId": "84968f6b-8bf5-4f3a-a953-8d7a283d1a63"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYL6mWBINKnN",
    "outputId": "0783159e-da46-4c11-b3b4-8699f2c294dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip nodule_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8qh9kz6zmiBt"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageOps, ImageDraw\n",
    "from PIL import ImageFilter\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vXgfkY1PrdBi"
   },
   "outputs": [],
   "source": [
    "!mkdir -p better_dataset/generated_masks\n",
    "!mkdir -p better_dataset/subtracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XpOV_vJPxRYQ"
   },
   "outputs": [],
   "source": [
    "!mkdir -p test/generated_masks\n",
    "!mkdir -p test/subtracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bT4nmvch1yRb"
   },
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "j0NEMJ-7rtvI"
   },
   "outputs": [],
   "source": [
    "with open('./nodule_dataset/nodulemetadata.json') as f:\n",
    "    nodule_dict = json.load(f)\n",
    "\n",
    "mask_path = \"./nodule_dataset/generated_masks\"\n",
    "subtract_path = \"./nodule_dataset/subtracts\"\n",
    "\n",
    "for nodule_name, (bbox, _, _) in nodule_dict.items():\n",
    "  nodule_name = f\"{nodule_name}.jpg\"\n",
    "\n",
    "  mask = Image.open(os.path.join(mask_path, nodule_name)).convert(\"1\")\n",
    "  mask = np.array(mask)\n",
    "\n",
    "  subtract = Image.open(os.path.join(subtract_path, nodule_name)).convert(\"L\")\n",
    "  subtract = np.array(subtract)\n",
    "  subtract[mask == False] = 0\n",
    "\n",
    "  subtract = Image.fromarray(subtract).convert(\"L\")\n",
    "  mask = Image.new(mode=\"L\", size=(SIZE, SIZE))\n",
    "  rect = ImageDraw.Draw(mask)\n",
    "  rect.rectangle(bbox, fill = 255)\n",
    "\n",
    "  mask.save(os.path.join(\"./better_dataset/generated_masks\", nodule_name))\n",
    "  subtract.save(os.path.join(\"./better_dataset/subtracts\", nodule_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3j3rwGyzxQpR"
   },
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "  width = random.randint(30, 100)\n",
    "  height = random.randint(30, 100)\n",
    "\n",
    "  xmin, ymin, xmax, ymax = SIZE//2 - width//2, SIZE//2 - height//2, SIZE//2 + width//2, SIZE//2 + height//2\n",
    "\n",
    "  mask = np.zeros(shape=(SIZE, SIZE))\n",
    "  mask[ymin:ymax, xmin:xmax] = 255\n",
    "\n",
    "  subtract = Image.new(mode=\"L\", size=(SIZE, SIZE))\n",
    "  mask = Image.fromarray(mask).convert(\"L\")\n",
    "\n",
    "  mask.save(os.path.join(\"./test/generated_masks\", f\"{i}.jpg\"))\n",
    "  subtract.save(os.path.join(\"./test/subtracts\", f\"{i}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Hkx90i4YYP9v"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # the generated image resolution\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 10\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"ddpm-nodules-128\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "D5jNZpgVCkJ8"
   },
   "outputs": [],
   "source": [
    "test_nodule_dir = \"./test/subtracts/\"\n",
    "test_mask_dir = \"./test/generated_masks/\"\n",
    "\n",
    "test_nodules = [Image.open(os.path.join(test_nodule_dir, name)) for name in os.listdir(test_nodule_dir) if name[-1] == \"g\"]\n",
    "test_masks = [Image.open(os.path.join(test_mask_dir, name)) for name in os.listdir(test_mask_dir) if name[-1] == \"g\"]\n",
    "\n",
    "test_nodules = test_nodules[:config.eval_batch_size]\n",
    "test_masks = test_masks[:config.eval_batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JSXL0txaYP9w"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "#config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
    "#dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "\n",
    "nodule_dir = \"./better_dataset/subtracts/\"\n",
    "mask_dir = \"./better_dataset/generated_masks/\"\n",
    "\n",
    "\n",
    "data_dict = {\"nodule\": [], \"mask\": []}\n",
    "\n",
    "for nodule_name in os.listdir(nodule_dir):\n",
    "  if nodule_name not in os.listdir(test_nodule_dir):\n",
    "    assert nodule_name in os.listdir(nodule_dir)\n",
    "    assert nodule_name in os.listdir(mask_dir)\n",
    "\n",
    "    data_dict[\"nodule\"].append(Image.open(os.path.join(nodule_dir, nodule_name)))\n",
    "    data_dict[\"mask\"].append(Image.open(os.path.join(mask_dir, nodule_name)))\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXDgPvXOYP9w"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "ðŸ’¡ You can find additional datasets from the [HugGan Community Event](https://huggingface.co/huggan) or you can use your own dataset by creating a local [`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder). Set `config.dataset_name` to the repository id of the dataset if it is from the HugGan Community Event, or `imagefolder` if you're using your own images.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "ðŸ¤— Datasets uses the [Image](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Image) feature to automatically decode the image data and load it as a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html) which we can visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "A-L98uBQYP9x",
    "outputId": "db4dabdd-503a-4be7-cd9c-bdd269f794a8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:4][\"nodule\"]):\n",
    "    axs[i].imshow(image, cmap='gray')\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[:4][\"mask\"]):\n",
    "    axs[i].imshow(image, cmap='gray')\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "P_Xqq-l1YP9x"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mask_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r67UKJ6eYP9x"
   },
   "source": [
    "Use ðŸ¤— Datasets' [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) method to apply the `preprocess` function on the fly during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8sLqpInzYP9x"
   },
   "outputs": [],
   "source": [
    "def transform(examples):\n",
    "    images = [image_preprocess(image.convert(\"RGB\")) for image in examples[\"nodule\"]] # doesn't matter cause we do gray scale in transform\n",
    "    masks = [mask_preprocess(image.convert(\"RGB\")) for image in examples[\"mask\"]]\n",
    "    return {\"images\": images, \"masks\": masks}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "test_nodules = torch.stack([image_preprocess(i) for i in test_nodules])\n",
    "test_masks = torch.stack([mask_preprocess(i) for i in test_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIdNwJ93YP9y"
   },
   "source": [
    "Feel free to visualize the images again to confirm that they've been resized. Now you're ready to wrap the dataset in a [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader) for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "NURefIVdZ767"
   },
   "outputs": [],
   "source": [
    "# custom loss\n",
    "\n",
    "def weighted_mse_loss(input, target, batch_weights):\n",
    "  batch_weighs = batch_weights.view(config.train_batch_size, 1, 1, 1)\n",
    "  return torch.mean(batch_weighs * (input - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "QBkYG9KkYP9y"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "47cYgj1zO2rw",
    "outputId": "f4222b0f-0baa-4c35-a97e-774ab382d4b3"
   },
   "outputs": [],
   "source": [
    "assert test_nodules.shape == test_masks.shape\n",
    "\n",
    "image = torch.randn(test_nodules.shape)\n",
    "\n",
    "image = image * test_masks + (1-test_masks) * test_nodules\n",
    "\n",
    "\n",
    "image = image[3]\n",
    "\n",
    "d = ((image + 1.0) * 127.5).type(torch.uint8)[0]\n",
    "transforms.functional.to_pil_image(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sbhMgLdYP9y"
   },
   "source": [
    "## Create a UNet2DModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2oJUJyCYP9y"
   },
   "source": [
    "Pretrained models in ðŸ§¨ Diffusers are easily created from their model class with the parameters you want. For example, to create a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d#diffusers.UNet2DModel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XvJDKvGeYP9y"
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpiwweuqYP9y"
   },
   "source": [
    "It is often a good idea to quickly check the sample image shape matches the model output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15F9HlITYP9y",
    "outputId": "d990439e-6e02-4793-e05d-825b331a187f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (image) shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
    "print(\"Input (image) shape:\", sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzLtgUKFnr16",
    "outputId": "8fefc8fb-cf3a-4c0f-e9c1-1e72ccf11d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (mask) shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "sample_mask = dataset[0][\"masks\"].unsqueeze(0)\n",
    "print(\"Input (mask) shape:\", sample_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KK3VKcD3YP9z",
    "outputId": "7213afb1-3830-4284-bee4-8b97fd9938ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufWJpQveYP9z"
   },
   "source": [
    "Great! Next, you'll need a scheduler to add some noise to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3Fvl88rYP9z"
   },
   "source": [
    "## Create a scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "natmDOp3YP9z"
   },
   "source": [
    "The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a *noise schedule* and an *update rule*.\n",
    "\n",
    "Let's take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the `sample_image` from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "rI1VSaezyZOP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "EmyLs7-pYP9z",
    "outputId": "baa08892-1990-46a7-c1c7-8880159c560d"
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(sample_image.shape)\n",
    "\n",
    "timesteps = torch.LongTensor([999])\n",
    "noisy_image = sample_mask * noise_scheduler.add_noise(sample_image, noise, timesteps) + (1 - sample_mask) * sample_image\n",
    "# noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "print(noisy_image.shape)\n",
    "\n",
    "d = ((noisy_image + 1.0) * 127.5).type(torch.uint8)[0]\n",
    "transforms.functional.to_pil_image(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "WF4DPxh6YP90"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ignore error some goofy ahh device thing\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "loss = F.mse_loss(noise_pred, noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm35aJT8YP90"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-z9x6vNYP90"
   },
   "source": [
    "By now, you have most of the pieces to start training the model and all that's left is putting everything together.\n",
    "\n",
    "First, you'll need an optimizer and a learning rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "fr-sunYIYP90"
   },
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POUPF3NMYP90"
   },
   "source": [
    "Then, you'll need a way to evaluate the model. For evaluation, you can use the [DDPMPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ddpm#diffusers.DDPMPipeline) to generate a batch of sample images and save it as a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "2rOBKm9CYP90"
   },
   "outputs": [],
   "source": [
    "from diffusers import DDIMPipeline\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new(\"L\", size=(cols * w, rows * h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "        masks=test_masks,\n",
    "        nodules=test_nodules\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoQbUmPsYP90"
   },
   "source": [
    "Now you can wrap all these components together in a training loop with ðŸ¤— Accelerate for easy TensorBoard logging, gradient accumulation, and mixed precision training. To upload the model to the Hub, write a function to get your repository name and information and then push it to the Hub.\n",
    "\n",
    "<Tip>\n",
    "\n",
    "ðŸ’¡ The training loop below may look intimidating and long, but it'll be worth it later when you launch your training in just one line of code! If you can't wait and want to start generating images, feel free to copy and run the code below. You can always come back and examine the training loop more closely later, like when you're waiting for your model to finish training. ðŸ¤—\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "kWGxbjpGu1r1"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "6VguMkt1YP91"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import HfFolder, Repository, whoami\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\"\n",
    "\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.push_to_hub:\n",
    "            repo_name = get_full_repo_name(Path(config.output_dir).name)\n",
    "            repo = Repository(config.output_dir, clone_from=repo_name)\n",
    "        elif config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch[\"images\"]\n",
    "            masks = batch[\"masks\"]\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "\n",
    "            #batch_weights = (config.image_size**2) / torch.sum(masks, dim=[1, 2, 3])\n",
    "\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = masks * noise_scheduler.add_noise(clean_images, noise, timesteps) + (1-masks) * clean_images\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0] * masks\n",
    "                noise *= masks\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDIMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                if config.push_to_hub:\n",
    "                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n",
    "                else:\n",
    "                    pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-WAN2nnYP99"
   },
   "source": [
    "Phew, that was quite a bit of code! But you're finally ready to launch the training with ðŸ¤— Accelerate's [notebook_launcher](https://huggingface.co/docs/accelerate/main/en/package_reference/launchers#accelerate.notebook_launcher) function. Pass the function the training loop, all the training arguments, and the number of processes (you can change this value to the number of GPUs available to you) to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "463aec55933d4695a0ca1679d6fbf669",
      "34a50e7dd8404e6faf84644335593471",
      "3b831a13bb75437d8997882a0a84f85b",
      "8a0802cf87b64974890bd3973487eda9",
      "61f228917dce4ac28d35ad685ab6a5a8",
      "88ac177114dc4b22a6deb967ee8426c6",
      "a84c7f34b822479b9c4d9e3de666d8fe",
      "45a4b1349fab4e60946852bd5198759a",
      "f451dd5ec2bf4ac5bba333dd60f69110",
      "4580578811744bcc8b103c7f75670214",
      "15249622f23441bfab91b345657ed550"
     ]
    },
    "id": "0Kq55tEOYP99",
    "outputId": "f8f2e2ec-5607-470e-9daa-8fe9a78810c5"
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzcfqVGzYP99"
   },
   "source": [
    "Once training is complete, take a look at the final ðŸ¦‹ images ðŸ¦‹ generated by your diffusion model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWIV6o75YP9-"
   },
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8iJ6hjXYP9-"
   },
   "source": [
    "Unconditional image generation is one example of a task that can be trained. You can explore other tasks and training techniques by visiting the [ðŸ§¨ Diffusers Training Examples](https://huggingface.co/docs/diffusers/main/en/tutorials/../training/overview) page. Here are some examples of what you can learn:\n",
    "\n",
    "* [Textual Inversion](https://huggingface.co/docs/diffusers/main/en/tutorials/../training/text_inversion), an algorithm that teaches a model a specific visual concept and integrates it into the generated image.\n",
    "* [DreamBooth](https://huggingface.co/docs/diffusers/main/en/tutorials/../training/dreambooth), a technique for generating personalized images of a subject given several input images of the subject.\n",
    "* [Guide](https://huggingface.co/docs/diffusers/main/en/tutorials/../training/text2image) to finetuning a Stable Diffusion model on your own dataset.\n",
    "* [Guide](https://huggingface.co/docs/diffusers/main/en/tutorials/../training/lora) to using LoRA, a memory-efficient technique for finetuning really large models faster."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15249622f23441bfab91b345657ed550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34a50e7dd8404e6faf84644335593471": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88ac177114dc4b22a6deb967ee8426c6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a84c7f34b822479b9c4d9e3de666d8fe",
      "value": "Epochâ€‡0:â€‡â€‡â€‡9%"
     }
    },
    "3b831a13bb75437d8997882a0a84f85b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45a4b1349fab4e60946852bd5198759a",
      "max": 444,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f451dd5ec2bf4ac5bba333dd60f69110",
      "value": 41
     }
    },
    "4580578811744bcc8b103c7f75670214": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45a4b1349fab4e60946852bd5198759a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "463aec55933d4695a0ca1679d6fbf669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_34a50e7dd8404e6faf84644335593471",
       "IPY_MODEL_3b831a13bb75437d8997882a0a84f85b",
       "IPY_MODEL_8a0802cf87b64974890bd3973487eda9"
      ],
      "layout": "IPY_MODEL_61f228917dce4ac28d35ad685ab6a5a8"
     }
    },
    "61f228917dce4ac28d35ad685ab6a5a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88ac177114dc4b22a6deb967ee8426c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a0802cf87b64974890bd3973487eda9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4580578811744bcc8b103c7f75670214",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_15249622f23441bfab91b345657ed550",
      "value": "â€‡41/444â€‡[00:29&lt;04:54,â€‡â€‡1.37it/s,â€‡loss=0.00769,â€‡lr=8.2e-6,â€‡step=40]"
     }
    },
    "a84c7f34b822479b9c4d9e3de666d8fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f451dd5ec2bf4ac5bba333dd60f69110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
